---
title: "mmultinom1"
author: "Zihao"
date: "2021-07-05"
output: workflowr::wflow_html
header-includes:
   - \usepackage{bbm}
   - \usepackage{amsfonts}
editor_options:
  chunk_output_type: console
---

## The `mmultinom` subproblem
In PMF using EM, to estimate $F$ we need to solve the subproblem (except we use $E(Z_{ijk})$ instead of $Z_{ijk}$):

\begin{align}
  & \sum_i Z_{ijk} \sim \mathrm{Pois}(\sum_i l_{ik} f_{jk}), \ \forall k \in [K]
\end{align}
If we do some scaling so that $\sum_j f_{jk} = 1$, and denote $m_{jk} := \sum_i Z_{ijk}, \ n_k := \sum_i l_{ik}, \ \beta_{jk} := f_{jk}$, we have can map it to the `mmultinom` problem defined below (not equivalent, but 1-1 between the MLEs)

\begin{align}
  & \mathbb{m}_k \sim \mathrm{Multinom}(n_k,  \mathbb{\beta}_{k}), \ \forall k \in [K]
\end{align}

The `mmultinom` problem becomes "hard" in high-dimensional setting (when $n_k < p$, say), and I expect MLE performs poorly in this setting. This scenario occurs in PMF when we use large $K$, or the total number of words is too small compared to $p$ (the data cannot support good estimation of desired number of topics)

The quality of the single MLE estimate $\hat{\beta}_{jk}$ only concerns the true $\beta_{jk}$, and $n_k$, as $n_k \hat{\beta}_{jk} \sim \mathrm{Bin}(n_k, \beta_{jk})$. We have $\mathrm{E}(\frac{\hat{\beta}_{jk}}{\beta_{jk}}) = 1$ &  $\mathrm{Var}(\frac{\hat{\beta}_{jk}}{\beta_{jk}}) = \frac{1 - \beta_{jk}}{n_k \beta_{jk}} \approx \frac{1}{n_k \beta_{jk}}$. Therefore, I expect the estimate to be good as long as $n_k \beta_{jk}$ is large, and bad when it's small. 

## Possible improvement by borrowing information 

* Perhaps in reality most words are not active in any of those topics (the same logic behind removing some words before applying models). Therefore, we can assume $\beta_{jk} = \beta_{j0} d_{jk}$. We can estimate $\beta_{j0}$ with all $\sum_k n_k$ occurences of words, and then estimate each $d_{jk}$ with $n_k$ occurences but assuming sparsity on $|d_{jk} - 1|$. 

* I haven't figured out a prior for $d_{jk}$ yet, but it's much easier if we use an "equivalent" Poisson model. 


## "Realistic" cases

* I use the fitted $L, F$ ([fastTopics on SLA](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_sla)) to generate the `mmultinom` problem below.  (dictionary size $p \approx 9000$). I change the difficulty of the problem by changing the document length and number of documents

* I find even in the most difficult case, MLE is sufficient for those "top words": whose corresponding $\beta_{jk} > 100/p$ (around $0.01$ in this example) (also implies they are the top 1% words). It gets bad for those with smaller $\beta_{jk}$. 

* I look at the other [document data](https://archive.ics.uci.edu/ml/datasets/bag+of+words) and speculate MLE would be fine for $K = 100$ for top 1% words. 

```{r}
rm(list = ls())
library(fastTopics)
source("code/mmultinom.R")
source("code/mmultinom_util.R")
source("code/util.R")
set.seed(123)
```

Load data
```{r}
dataname = "sla"
K = 20
fitted = readRDS(sprintf("output/fastTopics_fit/fit_%s_fastTopics_k%d.Rds",
                         dataname, K))
m = nrow(fitted$L)
```

Look at distribution of $\beta_{jk}$
```{r}
par(mfrow = c(2, 1))
#plot(ecdf(fitted$F), xlab = "beta_jk")
quantile(fitted$F, probs = seq(0.9, 1, 0.01))
hist(log10(fitted$F), xlab = "log10(beta_jk)", probability = TRUE)
hist(log10(fitted$F), xlab = "log10(beta_jk)", probability = TRUE,xlim = c(-5, 0), breaks = 30)
```
80% of $\beta_{jk}$ are just $0$ (set to a small value $< 10^{-15}$), as a result of MLE fit


### Use all documents, full document length
```{r fig.height=35, fig.width=20}
model = fitted
data_sim = TM2mmultinom(fit = model)
fit_mle = mmultinom.mle(M = data_sim$M, n = data_sim$n)

par(mfrow = c(K/4, 4))
ce = replicate(K, 0)
for(k in 1:K){
  p1 = data_sim$beta[k,]
  p2 = fit_mle[k, ]
  ce[k] = cross_entropy(p1 = p1, p2 = p2)
  
  idx <- (log10(p1) > -5)
  plot(p1[idx], log(p1[idx]) - log(p2[idx]),
       ylab = "log(beta) - log(betahat)", xlab = "beta",
       main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  abline(h = 0, col = "blue")
  # plot(log10(p1), log10(p1) - log10(p2),
  #      ylab = "log10(beta) - log10(betahat)", xlab = "log10(beta)",
  #      main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  # abline(h = 0, col = "blue")
}
round(ce, digits = 3)
sum(ce)
```


### Use less documents
Suppose we only observe 20% of the documents
```{r fig.height=35, fig.width=20}
model = fitted
idx = sample(x = 1:m, size = round(0.2*m))
model$L <- model$L[idx, ]
model$s <- model$s[idx]

data_sim = TM2mmultinom(fit = model)
fit_mle = mmultinom.mle(M = data_sim$M, n = data_sim$n)

par(mfrow = c(K/4, 4))
ce = replicate(K, NA)
for(k in 1:K){
  p1 = data_sim$beta[k,]
  p2 = fit_mle[k, ]
  ce[k] = cross_entropy(p1 = p1, p2 = p2)
  
  idx <- (log10(p1) > -5)
  plot(p1[idx], log(p1[idx]) - log(p2[idx]),
       ylab = "log(beta) - log(betahat)", xlab = "beta",
       main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  abline(h = 0, col = "blue")
  # plot(log10(p1), log10(p1) - log10(p2),
  #      ylab = "log10(beta) - log10(betahat)", xlab = "log10(beta)",
  #      main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  # abline(h = 0, col = "blue")
}
round(ce, digits = 3)
sum(ce)
```

### use shorter & less documents
I use half of the documents and 1/5 document length
```{r fig.height=35, fig.width=20}
model = fitted
idx = sample(x = 1:m, size = round(0.5*m))
model$L <- model$L[idx, ]
model$s <- model$s[idx]/5


data_sim = TM2mmultinom(fit = model)
fit_mle = mmultinom.mle(M = data_sim$M, n = data_sim$n)

par(mfrow = c(K/4, 4))
ce = replicate(K, 0)
for(k in 1:K){
  p1 = data_sim$beta[k,]
  p2 = fit_mle[k, ]
  ce[k] = cross_entropy(p1 = p1, p2 = p2)
  
  idx <- (log10(p1) > -5)
  plot(p1[idx], log(p1[idx]) - log(p2[idx]),
       ylab = "log(beta) - log(betahat)", xlab = "beta",
       main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  abline(h = 0, col = "blue")
  # plot(log10(p1), log10(p1) - log10(p2),
  #      ylab = "log10(beta) - log10(betahat)", xlab = "log10(beta)",
  #      main = sprintf("topic %d; n = %d", k, round(data_sim$n[k])))
  # abline(h = 0, col = "blue")
}

round(ce, digits = 3)
sum(ce)
```
