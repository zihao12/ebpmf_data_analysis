---
title: "kos_K20_ebpmf.alpha_v0.3.9"
author: "zihao12"
date: "2020-05-11"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

* I apply `ebpmf.alpha` (version 0.3.9) to [KOS dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/). I use $K = 20$. The data has $n = 3430,p = 6906$ and sparsity around $98$ percent. \

* Besides, I also apply to `PMF` (lee's, but I implemented a version for sparse data) to the same dataset with the same initialization. In each iteration, `ebpmf_bg` does two things: MLE for prior and updates posterior. The second part has almost the same computation as in `PMF`. 

### model
\begin{align}
    & X_{ij} = \sum_k Z_{ijk}\\
    & Z_{ijk} \sim Pois(l_{i0} f_{j0} l_{ik} f_{jk})\\
    & l_{ik} \sim g_{L, k}(.), f_{jk} \sim g_{F, k}(.) 
\end{align}
For details see [ebpmf_bg](https://github.com/stephenslab/ebpmf.alpha/blob/master/derivations/ebpmf_bg.pdf)

### prior options
I use gamma mixture $\sum_l \pi_{l} Ga(1/\phi_l, 1/\phi_l)$ as prior for both $L, F$.  Note that each grid component has $E = 1, Var = \phi_L$

### initialization
I initialized with 50 runs of `NNLM::nnmf` (`scd`). Then I used medians of each row of $L, F$ as $l_{i0}, f_{j0}$, and $l_{ik} = l^0_{ik}/l_{i0}, f_{jk} = f^0_{jk}/f_{j0}$. 

```{r include=FALSE}
#knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```


```{r}
library(pheatmap)
library(gridExtra)
source("code/misc.R")
source("code/util.R")

output_dir = "output/uci_BoW/v0.3.9/"
data_dir = "data/uci_BoW/"
model_name = "kos_ebpmf_bg_initLF50_K20_maxiter2000.Rds"
model_pmf_name = "kos_pmf_initLF50_K20_maxiter2000.Rds"
dict_name = "vocab.kos.txt"
data_name = "docword.kos.txt"

Y = read_uci_bag_of_words(file= sprintf("%s/%s",
			    data_dir,data_name))
model = readRDS(sprintf("%s/%s", output_dir, model_name))
model_pmf = readRDS(sprintf("%s/%s", output_dir, model_pmf_name))
dict = read.csv(sprintf("%s/%s", data_dir, dict_name), header = FALSE)[,1]
dict = as.vector(dict)

K = ncol(model_pmf$L)
L_pmf = model_pmf$L; F_pmf = model_pmf$F
L_bg = model$l0 * model$qg$qls_mean; F_bg = model$f0 * model$qg$qfs_mean
lf = poisson2multinom(L=L_bg,F=F_bg)
lf_pmf = poisson2multinom(L = L_pmf,F = F_pmf)
```

## ELBO and runtime

```{r}
plot(model$ELBO, xlab = "niter", ylab = "elbo")

## see when it "converges"
plot(model$ELBO[1:200], xlab = "niter", ylab = "elbo")
## ebpmf_bg runtime per iteration
model$runtime/length(model$ELBO)

## pmf runtime per iteration
model_pmf$runtime/length(model_pmf$log_liks)
```

## look at priors in `ebpmf_bg`

### $g_L$
```{r}
get_prior_summary(model$qg$gls)
```

### $g_F$
```{r}
get_prior_summary(model$qg$gfs)
```

## look at $s_k$ (`ebpmf_bg`)
$s_k := \sum_i l_i0 \bar{l}_{ik}$. I make $\sum_j f_{j0} = 1$ for interpretability. 
```{r}
d = sum(model$f0)
s_k = colSums(d * model$l0 * model$qg$qls_mean)
names(s_k) <- paste("Topic", 1:K, sep = "")
step = 5
for(i in 1:round(K/step)){
  print(round(s_k[((i-1)*step + 1):(i*step)]))
}
```

## what does background capture

### compared to rank-1 fit
Is the background very different from the rank-1 model? The rank-1 MLE has $l_{i0} \propto \sum_j X_{ij}$ and $f_{j0} \propto \sum_i X_{ij}$. Let's see if the fitted background model is close to it. 
```{r}
Y_cs = Matrix::colSums(Y)
Y_cs_scaled = Y_cs/sum(Y_cs)
f0_scaled = model$f0/sum(model$f0)
plot(f0_scaled, Y_cs_scaled)

Y_rs = Matrix::rowSums(Y)
Y_rs_scaled = Y_rs/sum(Y_rs)
l0_scaled = model$l0/sum(model$l0)
plot(l0_scaled, Y_rs_scaled)
```

### compared to median of `PMF` fit
```{r}
f0_pmf = apply(F_pmf, 1, median)
f0_pmf_scaled = f0_pmf/sum(f0_pmf)

l0_pmf = apply(L_pmf, 1, median)
l0_pmf_scaled = l0_pmf/sum(l0_pmf)

plot(f0_scaled, f0_pmf_scaled)
plot(l0_scaled, l0_pmf_scaled)
```

## Compare $L, F$ (in the context of PMF model)
See [plots](https://zihao12.github.io/ebpmf_data_analysis/kos_K20_v0.3.9_compare_LF.pdf). \

Note: I scale them as below
```{r eval=FALSE}
## scale L, F so that colSums(F) = 1
L_pmf = L_pmf %*% diag(colSums(F_pmf))
F_pmf = F_pmf %*% diag(1/colSums(F_pmf))

L_bg = L_bg %*% diag(colSums(F_bg))
F_bg = F_bg %*% diag(1/colSums(F_bg))
```


## look at top words for topics
See [plots](https://zihao12.github.io/ebpmf_data_analysis/kos_K20_v0.3.9_topic_words.pdf). \

Note: for each topic: \

* the first row selects the top words from $\bar{f}_{Jk}$, and show them in $\bar{f}$(bg) and $f$ (PMF) respectively. \

* The second row shows the top words from $f_{J0}0\bar{f}_{Jk}$ (bg) and $f_{Jk}$ (PMF)\

* The third row transforms $f_{J0}0\bar{f}_{Jk}$ (bg) and $f_{Jk}$ into multinomial model and show their top words.

### take a closer look at some top words
In topic 1, the word **upenn** **seamus** are the top words in $\bar{f}_{J1}$, and **november**, **bush** are the top words in $f_{J0}\bar{f}_{J1}$. Let's see their correspoding $f_{j0}$
```{r}
d = Matrix::summary(Y)
## `upenn`
idx = which(dict == "upenn")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]


## `seamus`
idx = which(dict == "seamus")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]


## `november`
idx = which(dict == "november")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]


## `bush`
idx = which(dict == "bush")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]

```


## "similar topics"
By looking at top words, I find topic 1 & 12 have some similarities. 
([danielua](https://m.dailykos.com/stories/1052656) and [misterajc](https://www.dailykos.com/stories/2012/3/31/1079533/-BREAKING-Romney-names-running-mate) are both names of authors)
```{r}
f1 = model$qg$qfs_mean[, 1]
f2 = model$qg$qfs_mean[,12]
plot(f1, f2)

## those important in topic 1 but not so in topic 12
## note there are many key words in topic 1!
idx = which(f1 > 200 & f2 < 1)
dict[idx]

## those important in topic 12 but not so in topic 1
idx = which(f1 < 1 & f2 > 200)
dict[idx]


## those important in both topics
idx = which(f1 > 100 & f2 > 100)
dict[idx]

```



<!-- ## Look at quantile of topics -->

<!-- ### quantile of $f_{J0}$ (`ebpmf_bg`) -->
<!-- ```{r} -->
<!-- f = model$f0 -->
<!-- probs = seq(0, 1, 0.002) -->
<!-- plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",0)) -->
<!-- ``` -->

<!-- ### quantile of $f_{Jk} (k > 0)$ (`ebpmf_bg`) -->

<!-- ```{r fig.width=14, fig.height=14} -->
<!-- K = length(model$qg$gls) -->
<!-- par(mfrow = c(5,4)) -->
<!-- for(k in 1:K){ -->
<!--   f = model$qg$qfs_mean[,k] -->
<!--   probs = seq(0, 1, 0.002) -->
<!--   plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",k)) -->
<!-- } -->
<!-- ``` -->


<!-- ### quantile of $f_{J0} f_{Jk}$ (`ebpmf_bg`) (scaled to multinom) -->
<!-- ```{r fig.width=14, fig.height=14} -->
<!-- lf = poisson2multinom(F = model$f0 * model$qg$qfs_mean, -->
<!--                  L = model$l0 * model$qg$qls_mean) -->

<!-- K = length(model$qg$gls) -->
<!-- par(mfrow = c(5,4)) -->
<!-- for(k in 1:K){ -->
<!--   f = lf$F[,k] -->
<!--   probs = seq(0, 1, 0.002) -->
<!--   plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",k)) -->
<!-- } -->
<!-- ``` -->

<!-- ### quantile of $f_{Jk}$ (`PMF`) (scaled to multinom) -->
<!-- ```{r fig.width=14, fig.height=14} -->
<!-- lf_pmf = poisson2multinom(F = model_pmf$F, L = model_pmf$L) -->
<!-- par(mfrow = c(5,4)) -->
<!-- for(k in 1:K){ -->
<!--   f = lf_pmf$F[,k] -->
<!--   probs = seq(0, 1, 0.002) -->
<!--   plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",k)) -->
<!-- } -->
<!-- ``` -->



