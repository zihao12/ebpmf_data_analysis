---
title: "investigate_F_proj"
author: "Zihao"
date: "2021-06-07"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---


## Summary

* In Poisson Matrix Factoriation applications where $n \ll p$, I find in many (possibly most) cases, Factors (either truth or fitted) approximately lie in the convex cone of rows of data. In other word, factors are weighted (non-negative) averages of rows of data. It generalizes the "anchor document" assumption. 
* If this assumption is reasonable for many applications, we might be able to develop faster and more reliable (even identifiable) Poisson Matrix Factorization methods. For example, if we let $f_k$ be parameterized by $X^t a_k$ we only need $n$ instead of $p$ parameters. 

## Motivation

I simulated some "fat and short" matrices ($\text{dim}(X) = (n, p); n \ll p$) for Poisson Matrix Factorization experiments. In many cases I find we can estimate factor $F$ very well even with very small number of samples say $n = 50, p = 500$. It turns out that the same small subset probably contains "anchor samples" (pure samples of a topic) so it has most structural information. 

```{r}
rm(list = ls())
library(NNLM)
library(nnls)
source("code/smallsim2_functions.R")
source("code/misc.R")
source("code/util.R")

set.seed(123)
```

## An example where a small sample size is enough to recover true topics

Below we can estimate $F$ well with $\text{dim}(X) = (60, 1000)$. 
```{r}
n <- 300
m <- 1000
k <- 3
n_sample = 60
doc_len = m

S <- 15*diag(k) - 2
F <- simulate_factors(m = m, k = k)
L <- simulate_loadings(n,k,S)
s <- simulate_sizes(n = n, doc_len = doc_len)
X <- simulate_multinom_counts(L,F,s)
```

```{r warning=FALSE, message=FALSE}
fit0 = nnmf(A = X[1:n_sample, ], k = k, loss = "mkl", method = "lee", max.iter = 50)

fit_small = nnmf(A = X[1:n_sample, ], k = k, loss = "mkl", method = "lee", init = list(H = fit0$H), max.iter = 200)
fit_big = nnmf(A = X, k = k, loss = "mkl", method = "lee", init = list(H = fit0$H), max.iter = 200)

fit_small.m = get_multinom_from_pnmf(F = t(fit_small$H), L = fit_small$W)
fit_big.m = get_multinom_from_pnmf(F = t(fit_big$H), L = fit_big$W)

idx_small = match_topics(F1 = F, F2 = fit_small.m$F)
idx_big = match_topics(F1 = F, F2 = fit_big.m$F)
```

Fitted (small data) VS truth
```{r}
model = fit_small.m
idx = idx_small
par(mfrow = c(3,2))
compare_truth_fitted(model, idx, L, F)
```

Fitted (bigger data) VS truth
```{r}
model = fit_big.m
idx = idx_big
par(mfrow = c(3,2))
compare_truth_fitted(model, idx, L, F)
```


* It's really amazing that with $(n, p) = (60, 1000)$ we can recover truth so well. 
* One reason I think, is the existence of "anchor" samples that's a pure representative of a topic ($l_{i} = (1, 0, 0)$ say). And the small subset of samples contain those anchor samples for each topic. 
* If those "anchor" samples also have enough obserevations (number of multinomial draws from the underlying prob vector) , then it's possible that a small subset of samples contain most of the structural information. 

```{r}
par(mfrow = c(2,2))
for(i in 1:k){
  hist(L[,i], xlab = "loading", main = sprintf("topic %d", i))
}
```

## Factors are contained in the convex cone of $\text{row}(X)$

In fact, I speculate each true topic can be represented as a weighted (non-negative) average of samples: $f_k = X^{T} a_k, a_k \geq 0$. This is more general than the "anchor sample" assumption. 

Let's see if this speculation is correct. I minimize $| X^{T} a_k - f_k|_2^2, \  \text{s.t.} a_k \geq 0$. Here **X is the small submatrix of (60, 1000)** and **f_k is true factor** I call the fitted $X^{T} a_k$ as $f_{proj}$. I want to see if the residuals are very small (solved using `nnls` library)
```{r}
par(mfrow = c(2, 2))
for(i in 1:k){
  f = F[,i]
  f_proj  = nnls(A = t(X[1:n_sample,]), b = f)
  plot(f, f_proj$fitted, ylab = "f_proj", 
       main = sprintf("max abs(resid) :\n %.3f", 
                      max(abs(f_proj$residuals))))
  abline(a = 0, b = 1, col = "blue")
}
```

Indeed $f, f_{proj}$ are quite similar, at least in the most distinguishing words. Therefore the small submatrix contain most of the "useful" information!! 

## On real dataset
This is a document dataset I fitted before: https://zihao12.github.io/ebpmf_data_analysis/kos_K20_ebpmf.alpha_v0.3.8.html
```{r cache=TRUE}
Y = read_uci_bag_of_words("data/uci_BoW/docword.kos.txt")
fitted = readRDS("output/uci_BoW/v0.3.9/kos_pmf_initLF50_K20_maxiter5000.Rds")
```

First, let's see if the fitted factors $F_k$'s can be represented as weighted sums of rows of $X$. Below we can see it's approximately true. 
```{r cache=TRUE}
par(mfrow = c(2,2))
for(i in c(1,3,12,20)){
  f = fitted$F[,i]
  f_proj = nnls(A = t(as.matrix(Y)), b = f)
  plot(f, f_proj$fitted, ylab = "f_proj", 
         main = sprintf("max abs(resid) :\n %.3f", 
                        max(abs(f_proj$residuals))))
  abline(a = 0, b = 1, col = "blue")
}
```

What about we subsample 1/3 of the samples. Can we still have fitted factors in the cone of rows of $X$? It's not that different (among the most distinguishing words) from using the whole dataset!
```{r cache=TRUE}
set.seed(123)
idx = sample(x = 1:nrow(Y), size = round(nrow(Y)/3), replace = FALSE)
par(mfrow = c(2,2))
for(i in c(1,3,12,20)){
  f = fitted$F[,i]
  f_proj = nnls(A = t(as.matrix(Y[idx, ])), b = f)
  plot(f, f_proj$fitted, ylab = "f_proj", 
         main = sprintf("max abs(resid) :\n %.3f", 
                        max(abs(f_proj$residuals))))
  abline(a = 0, b = 1, col = "blue")
}
```

## An example where recovering true topics is hard

We know in general topic modeling is hard. Below I just let $L, F$ be from uniform, and also decrease the length of each document. 
```{r}
set.seed(123)
n <- 300
m <- 1000
k <- 3
n_sample = 60
doc_len = round(m/6)

F <- matrix(runif(n = m*k), ncol = k)
L <- matrix(runif(n = n*k), ncol = k)
F <- normalize.cols(F)
L <- t(normalize.cols(t(L)))

s <- simulate_sizes(n = n, doc_len = doc_len)
X <- simulate_multinom_counts(L,F,s)
```

```{r warning=FALSE, message=FALSE}
fit0 = nnmf(A = X[1:n_sample, ], k = k, loss = "mkl", method = "lee", max.iter = 50)

fit_small = nnmf(A = X[1:n_sample, ], k = k, loss = "mkl", method = "lee", init = list(H = fit0$H), max.iter = 200)
fit_big = nnmf(A = X, k = k, loss = "mkl", method = "lee", init = list(H = fit0$H), max.iter = 200)

fit_small.m = get_multinom_from_pnmf(F = t(fit_small$H), L = fit_small$W)
fit_big.m = get_multinom_from_pnmf(F = t(fit_big$H), L = fit_big$W)

idx_small = match_topics(F1 = F, F2 = fit_small.m$F)
idx_big = match_topics(F1 = F, F2 = fit_big.m$F)
```

Fitted (small data) VS truth
```{r}
model = fit_small.m
idx = idx_small
## matched topic index
idx
par(mfrow = c(3,2))
compare_truth_fitted(model, idx, L, F)
```

Fitted (bigger data) VS truth
```{r}
model = fit_big.m
idx = idx_big
## matched topic index
idx
par(mfrow = c(3,2))
compare_truth_fitted(model, idx, L, F)
```

We can see even bigger dataset is not enough to recover the truth

Below let's see if truth `f_k`'s lie in the cone of X rows (bigger data). We can see below it's quite messy so the data does not contain all the information of the underllying structure.  
```{r}
par(mfrow = c(2, 2))
for(i in 1:k){
  f = F[,i]
  f_proj  = nnls(A = t(X), b = f)
  plot(f, f_proj$fitted, ylab = "f_proj", 
       main = sprintf("max abs(resid) :\n %.3f", 
                      max(abs(f_proj$residuals))))
  abline(a = 0, b = 1, col = "blue")
}
```



