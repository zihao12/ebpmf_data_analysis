---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

Welcome to my research website.


## Model:

* [Model description for ebpmf-wbg](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_description)

* [Derivations](https://github.com/stephenslab/ebpmf.alpha/tree/master/derivations)

* [Presentation at Stephenslab group meeting](https://zihao12.github.io/ebpmf_data_analysis/PMF_WBG_presentation.pdf)

<!-- Data preprocessing demo:\ -->

<!-- * [process SLA dataset](https://zihao12.github.io/ebpmf_data_analysis/data_preprocessing_sla)\ -->

## Data analysis results:

The goal is to find situations where our EB approach can imporve upon MLE (or Bayesian approaches like LDA). Some datsets used are: [sla](https://zihao12.github.io/ebpmf_data_analysis/data_preprocessing_sla) ...

* On [simulated data1](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2) (and [more](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_more)) & [simulated data2](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2) We can see our EB approach has the potential to beat MLE in terms of "[False Discoveries](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2#pmf_bg_from_truth)" of important words & documents. However the requires initialization from close to the truth. So I hope that we can find applications where PMF fit is basically right but can be refined much better with EB approach, like in [simulated data1](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2)

* I first did [fastTopics_on_sla](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_sla). I find estimating $F$ seems easier than estimating $L$. Then on [simulated data from sla](https://zihao12.github.io/ebpmf_data_analysis/prepare_sla_sim): I compared [MLE vs ebpmf-wbg](https://zihao12.github.io/ebpmf_data_analysis/sla_simulated_compare_mle_eb) and find EB gives much better estimate of $\hat{L}$.

* About asymmetry of $L, F$: [fastTopics_on_sla2](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_sla2), [fastTopics_on_droplet](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_droplet)

* [On real data](https://zihao12.shinyapps.io/topicview-app/) (in development); 

<!-- * Compare MLE vs EB fits: [sla](https://zihao12.github.io/ebpmf_data_analysis/sla_compare_mle_eb), [nips](https://zihao12.github.io/ebpmf_data_analysis/nips_compare_mle_eb) -->


## Other stuff

### paper reading

* There are several interesting variants of [LDA](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf): [Correlated Topic Model](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-1/issue-1/A-correlated-topic-model-of/10.1214/07-AOAS114.full), [sparse additive generative models of text (SAGE)](http://www.cs.cmu.edu/~epxing/papers/2011/Eisenstein_Ahmed_Xing_ICML11.pdf), [Structural Topic Model](https://scholar.princeton.edu/files/bstewart/files/stmnips2013.pdf). Here are [my notes](https://drive.google.com/file/d/1W0v2oUS50DgBOVw8Ch5CjBKDELoVjspb/view?usp=sharing). The "sparsity" assumption of SAGE is basically the same as in ours, but imposed using different priors. 

* Our current optimization approach is VBEM (EB), which is slow to converge and can get stuck at bad local optimal. Some [attempted alternatives](https://users.rcc.uchicago.edu/~aksarkar/singlecell-ideas/hpmf.html). One key problem is compute gradient for $E_q log(X | L, F)$ and [Monte Carlo Gradient Estimation in Machine Learning](https://www.jmlr.org/papers/volume21/19-346/19-346.pdf) suggests some methods applicable here. 


### cone-NMF 
(the Frobenius norm case is the same as [convex-NMF](https://people.eecs.berkeley.edu/~jordan/papers/ding-li-jordan-pami.pdf)):

* First, I found that our regular PMF solution is basically inside $\text{cone}(X)$ where each column of $X$ is a sample: [cone_pmf1](https://zihao12.github.io/ebpmf_data_analysis/cone_pmf1.html)

* Then I derived and implemented the cone NMF for Frobeneus norm: [cone_nmf_l2](https://zihao12.github.io/ebpmf_data_analysis/cone_nmf_l2) . I note that fitted $B, W^T$ are almost identical. I fitted on real data to see if it's still the case: [cone on kos data](https://zihao12.github.io/ebpmf_data_analysis/cone_nmf_f_3)

* I find an example where cone NMF can improve the PMF fit: [cone_NMF_l2_2](https://zihao12.github.io/ebpmf_data_analysis/cone_NMF_l2_2)

* I also investigated direct estimates of word-word covariance matrix: [multinom_sampling](https://zihao12.github.io/ebpmf_data_analysis/multinom_sampling)


### `mmultinom`
I consider the subproblem in the estimation of $F$: [mmultinom1](https://zihao12.github.io/ebpmf_data_analysis/mmultinom1). I think borrowing information across topics to estimate $F$ (e.g. background model) benefits those less important words the most, whereas for the important words, MLE probably suffices (in a usual dataset not crazily sparse)

### Anchor-word based topic modeling

* I find [paper](https://www.cs.cornell.edu/~bindel/papers/2015-nips.pdf) & [paper](https://moontae.people.uic.edu/papers/pdfs/Moontae_Lee-EMNLP2019.pdf) gives a clear probabilistic framework for anchor-word based topic models, and they have the rather recent implementations. I wrote a [study note](https://drive.google.com/file/d/1QU35h0JyrWuANTKFKC-EAgTVb2w5LjEh/view?usp=sharing) & [study note](https://drive.google.com/file/d/1R9kdE8u8wPpNQgk40YMawHrAuKJtFfTC/view?usp=sharing) based on the two papers and the [seminal paper](https://arxiv.org/abs/1204.1956) & [seminal paper](https://arxiv.org/abs/1212.4777)

* My experiments using jupyter notebook can be seen [here](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/HEAD)

* So far I can say this type of method is very fast, but can give very bad results compared to MLE. 

* The bottleneck step for estimation is `recoverS` step:  [here](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/8814ae04b701fe16397b44ce706033fdefc3936a?urlpath=lab%2Ftree%2Fexperiments%2Fdiagnostic1.ipynb). In this step quality for estimation of the entire $\bar{C}$ matrix is important, and we can't obtain estimates good enough in many practical problems, without truncating dictionary. In the [example](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/8814ae04b701fe16397b44ce706033fdefc3936a?urlpath=lab%2Ftree%2Fexperiments%2Finvestigate_recoverS.ipynb) and [example](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/8814ae04b701fe16397b44ce706033fdefc3936a?urlpath=lab%2Ftree%2Fexperiments%2Finvestigate_recoverS2.ipynb),  we can see how the estimation error for $\bar{C}$ affects the geometry and thus leads to wrongly selected anchor words; I also show how we can improve the estimate using `sinkhorn + truncated SVD` idea. I explore this further in this  [experiment](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/8814ae04b701fe16397b44ce706033fdefc3936a?urlpath=lab%2Ftree%2Fexperiments%2Fsinkhorn1.ipynb)

* Another modeling choice is to use the first moment directly (instead of forming the word-word co-occurrence matrix). I experimented [here](https://mybinder.org/v2/gh/zihao12/pyJSMF-RAW/8814ae04b701fe16397b44ce706033fdefc3936a?urlpath=lab%2Ftree%2Fexperiments%2Ffirst_moment%3F.ipynb). Directly using counts for anchor words to represent loading is very bad; can be improved by `sinkhorn + truncated SVD`. Still it's much worse than MLE. 


<!-- * I find the algorithm can recover $F$ really well ($A$ is not as good though), even though the identified "anchor words" do not satisfy the anchor-word assumptions in [the small experiment](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/smallsim_experiment1.ipynb). The reason is that the rows of the identified "anchor words" are very similar to the rows of the true "anchor words" in [here](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/smallsim_experiment3.ipynb) -->

<!-- * In a [more realistically simulated dataset](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/sla_multinomial1.ipynb) & and a [harder version](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/sla_multinomial3.ipynb), we can see the algorithm can  -->
<!--     * get okay estimate for $F$ but poor estimate for $A$ (picked the wrong anchor words) -->
<!--     * get perfect result if we know the true $C$ (the dataset happens to satify the anchor word assumption) (I also looked at [harder case where $k = 20$](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/output/sla_multinomial_knowC_k20.out); we also gets perfect recovery; anchor-word assumption also holds true. Since the true $L, F$ are MLE fit on real data, it seems anchor word assumption is reasonable in the real data... though estimation of $C$ would be even harder).   -->
<!--     * if given correct anchor words, the estimate of $A$ improves a lot -->

<!-- * In this [small experiment](https://github.com/zihao12/pyJSMF-RAW/blob/master/experiments/smallsim_experiment4.ipynb) we can see reducing dictionary size can improve the estimate a lot. The main issue with high dimension is identifying correct anchor rows.   -->

<!-- * The weakness of anchor-word based methods is that they are not very robust:it requires estimatation of the word co-occurence probability matrix $C_{ij} = P(X_1 = w_i, X_2 = w_j)$, which is high dimensional and built from very sparse dataset; results crucially depend on only $K$ rows of $C$. In practice the we often find the wrong anchor rows with poor estimates for $C_{s_k, k}$. Why? We have different confidence levels for estimates of rows of $C$, but after normalizing rows of $C$ we don't use the different confidence levels when doing vertex hunting.  -->

<!-- * Possible improvements: we have $p$ points $\hat{C}_i$ in the $p$-simplex, each is an observation of a point in the $p$-simplex, $C_i$. The anchor word assumption says there are $K$ rows of $C_i$ whose convex space is the same as convex space of $C$. Can we jointly estimate $C, S, F, A$?  -->


### Incorporate covariates using background model
* [ebpmf_wbg2_subject](https://zihao12.github.io/ebpmf_data_analysis/experiment_ebpmf_wbg_subject)











