---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

Welcome to my research website.


## Model:

* [Model description for ebpmf-wbg](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_description)

* [Derivations](https://github.com/stephenslab/ebpmf.alpha/tree/master/derivations)

* [Presentation at Stephenslab group meeting](https://zihao12.github.io/ebpmf_data_analysis/PMF_WBG_presentation.pdf)

<!-- Data preprocessing demo:\ -->

<!-- * [process SLA dataset](https://zihao12.github.io/ebpmf_data_analysis/data_preprocessing_sla)\ -->

## Data analysis results:

The goal is to find situations where our EB approach can imporve upon MLE (or Bayesian approaches like LDA). Some datsets used are: [sla](https://zihao12.github.io/ebpmf_data_analysis/data_preprocessing_sla) ...

* On [simulated data1](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2) (and [more](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_more)) & [simulated data2](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2) We can see our EB approach has the potential to beat MLE in terms of "[False Discoveries](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2#pmf_bg_from_truth)" of important words & documents. However the requires initialization from close to the truth. So I hope that we can find applications where PMF fit is basically right but can be refined much better with EB approach, like in [simulated data1](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2)

* I first did [fastTopics_on_sla](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_sla). I find estimating $F$ seems easier than estimating $L$. Then on [simulated data from sla](https://zihao12.github.io/ebpmf_data_analysis/prepare_sla_sim): I compared [MLE vs ebpmf-wbg](https://zihao12.github.io/ebpmf_data_analysis/sla_simulated_compare_mle_eb) and find EB gives much better estimate of $\hat{L}$.

* About asymmetry of $L, F$: [fastTopics_on_sla2](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_sla2), [fastTopics_on_droplet](https://zihao12.github.io/ebpmf_data_analysis/fastTopics_on_droplet)

* [On real data](https://zihao12.shinyapps.io/topicview-app/) (in development); 

<!-- * Compare MLE vs EB fits: [sla](https://zihao12.github.io/ebpmf_data_analysis/sla_compare_mle_eb), [nips](https://zihao12.github.io/ebpmf_data_analysis/nips_compare_mle_eb) -->


## Other stuff

### paper reading

* There are several interesting variants of [LDA](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf): [Correlated Topic Model](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-1/issue-1/A-correlated-topic-model-of/10.1214/07-AOAS114.full), [sparse additive generative models of text (SAGE)](http://www.cs.cmu.edu/~epxing/papers/2011/Eisenstein_Ahmed_Xing_ICML11.pdf), [Structural Topic Model](https://scholar.princeton.edu/files/bstewart/files/stmnips2013.pdf). Here are [my notes](https://drive.google.com/file/d/1W0v2oUS50DgBOVw8Ch5CjBKDELoVjspb/view?usp=sharing). The "sparsity" assumption of SAGE is basically the same as in ours, but imposed using different priors. 

* Our current optimization approach is VBEM (EB), which is slow to converge and can get stuck at bad local optimal. Some [attempted alternatives](https://users.rcc.uchicago.edu/~aksarkar/singlecell-ideas/hpmf.html). One key problem is compute gradient for $E_q log(X | L, F)$ and [Monte Carlo Gradient Estimation in Machine Learning](https://www.jmlr.org/papers/volume21/19-346/19-346.pdf) suggests some methods applicable here. 


### cone-NMF 
(the Frobenius norm case is the same as [convex-NMF](https://people.eecs.berkeley.edu/~jordan/papers/ding-li-jordan-pami.pdf)):

* First, I found that our regular PMF solution is basically inside $\text{cone}(X)$ where each column of $X$ is a sample: [cone_pmf1](https://zihao12.github.io/ebpmf_data_analysis/cone_pmf1.html)

* Then I derived and implemented the cone NMF for Frobeneus norm: [cone_nmf_l2](https://zihao12.github.io/ebpmf_data_analysis/cone_nmf_l2) . I note that fitted $B, W^T$ are almost identical. I fitted on real data to see if it's still the case: [cone on kos data](https://zihao12.github.io/ebpmf_data_analysis/cone_nmf_f_3)

* I find an example where cone NMF can improve the PMF fit: [cone_NMF_l2_2](https://zihao12.github.io/ebpmf_data_analysis/cone_NMF_l2_2)

* I also investigated direct estimates of word-word covariance matrix: [multinom_sampling](https://zihao12.github.io/ebpmf_data_analysis/multinom_sampling)


### `mmultinom`
I consider the subproblem in the estimation of $F$: [mmultinom1](https://zihao12.github.io/ebpmf_data_analysis/mmultinom1)















