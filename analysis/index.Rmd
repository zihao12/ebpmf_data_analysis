---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---


# Model:

## Poisson Matrix Factorization (PMF)

* Suppose we observe count matrix $X \in R_{+}^{n \times p}$. In topic modeling context, $X_{ij}$ means the number occurrences of word $i$ in document $j$; in gene expression data, it means the number of occurrences in gene $i$ in cell $j$. 
* PMF assumes its underlying mean matrix can be decomposed into two low dimensional, non-negative matrices $L F^T$ where $L \in R^{n \times K}, F \in R^{p \times K}$. Each column of $F$ is called a factor/topic, representing the importance of a word/gene in a topic/biological process.  Each row of $L$ is the membership vector for each sample (document/cell) $i$, representing the importances of topics/bioligical processes for the sample.  
* The model can be written as:
\begin{align}
  & X_{ij} \sim \text{Pois}(\lambda_{ij})\\
  & \lambda_{ij} = \sum_{k} l_{ik} f_{jk}\\
  & l_{ik}, f_{jk} \geq 0
\end{align}

* To estimate $L, F$ we maximize the likelihood of the model above, through various types of coordinate descent methods. 

* Sometimes the data is very sparse so MLE tends to overfit. Therefore we need to find ways to regularize $L, F$. 

## Adaptive penalization with Empirical Bayes approach (EB)

* If we simply add a penalization function on $F$, we either have only one penalization parameter for all $k$ factors, or need to tune $k$ penalization parameters simultaneously. The same is true for regularizing $L$. 
* With EB approach, we can flexibly incorporate penalization in a data-adaptive fashion:
\begin{align}
  & X_{ij} \sim \text{Pois}(\lambda_{ij})\\
  & \lambda_{ij} = \sum_{k} l_{ik} f_{jk}\\
  & l_{ik} \sim g_{L, k}, f_{jk} \sim g_{F, k}\\
  & g_{L, k} \in \mathcal{G}_L, g_{F, k} \in \mathcal{G}_F
\end{align}
* We estimate $\hat{g}$'s with marginal MLE (so the prior/penalization is adaptive to data); then compute posterior for $L, F$. (The actual [derivations](https://github.com/stephenslab/ebpmf.alpha/tree/master/derivations) uses data augmentation and mean-field variational inference)
* This is a rather general framework (implemented in `ebpmf` function) for imposing penalizations. Next let's discuss how to impose "sparsity" assumption. 


## "Sparsity" assumption with EB + "background model"
* In topic modeling or gene expression data analysis, a "sparse" model is more interpretable and probably generalizes better than MLE with limited data. However, the usual defintion of sparsity (parameters have few nonzero elements) is not suitable for the applications here (unreasonable to think a topic does not allow a certain word), and easily cause numerical issue (a nonzero count has 0 likelihood if the mean is $0$).
* A more natural definition of sparsity: each word/gene has a background frequency (call the $p$-vector $f_0$); each factor $f_k$ encodes the deviation from the background frequency. Then "sparse" factor means very few words/genes deviate from $f_0$:

\begin{align}
  & X_{ij} \sim \text{Pois}(\lambda_{ij})\\
  & \lambda_{ij} = f_{j0} \sum_{k} w_k l_{ik} f_{jk}\\
  & l_{ik} \sim \text{Ga}(a_k, b_k) \\
  & f_{jk} \sim g_{F, k}\\
  & g_{F, k} \in \mathcal{G}_F
\end{align}

* Here each prior $g_{F, k}$ has most weights around $1$, encouraging elements of $f_k$'s to be close to 1 ("sparse"). 

### choice of prior family

We use mixture of gamma priors (each gamma prior has mean $1$ and variance $\phi_l$):

\begin{align}
  \mathcal{G}_F = \{g: g(.) = \sum_{l} \pi_l \mathrm{Ga}(.; 1/\phi_l, 1/\phi_l)\}
\end{align}
Here $\{\phi_l\}_l$ are pre-specified grids and the free parameters are weights $\{\pi_l\}_l$. If the estimated weights are concentrated around small $\phi_l$'s then it means very few words/genes deviate from the background;if the estimated weights allow very large $\phi_l$, then more words/genes can deviate from the backrground. (More [discussion](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_description)). This model is implemented as `ebpmf_wbg2` function. 


# Data Analysis


* On [simulated data1](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2_2) & [simulated data2](https://zihao12.github.io/ebpmf_data_analysis/ebpmf_wbg_simulation_big2), we can see `pmf` overfits the data and interprets noise as signals; whereas if initialized properly, `ebpmf_wbg` recovers the true sparse structure very well. 

* On a text dataset [sla](https://zihao12.github.io/ebpmf_data_analysis/data_preprocessing_sla) I compared [pmf vs ebpmf-wbg](https://zihao12.github.io/ebpmf_data_analysis/sla_simulated_compare_mle_eb) and find our approach gives much better estimate of both $L, F$.

* An [interactive view](https://zihao12.shinyapps.io/topicview-app/)  of the learned models in 3 text datasets.

# Future work
* `ebpmf`-type methods rely on good initializations: when initialization is good enough, it can refine the solution to be closer to the truth; but when initialization is bad, it can't recover good solutions. Therefore we need a good initialization strategy. 









