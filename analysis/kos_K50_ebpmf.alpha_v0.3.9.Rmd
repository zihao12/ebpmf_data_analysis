---
title: "kos_K50_ebpmf.alpha_v0.3.9"
author: "zihao12"
date: "2020-05-11"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

* I apply `ebpmf.alpha` (version 0.3.9) to [KOS dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/). I use $K = 50$. The data has $n = 3430,p = 6906$ and sparsity around $98$ percent. \

* Besides, I also apply to `PMF` (lee's, but I implemented a version for sparse data) to the same dataset with the same initialization. In each iteration, `ebpmf_bg` does two things: MLE for prior and updates posterior. The second part has almost the same computation as in `PMF`. 

### model
\begin{align}
    & X_{ij} = \sum_k Z_{ijk}\\
    & Z_{ijk} \sim Pois(l_{i0} f_{j0} l_{ik} f_{jk})\\
    & l_{ik} \sim g_{L, k}(.), f_{jk} \sim g_{F, k}(.) 
\end{align}
For details see [ebpmf_bg](https://github.com/stephenslab/ebpmf.alpha/blob/master/derivations/ebpmf_bg.pdf)

### prior options
I use gamma mixture $\sum_l \pi_{l} Ga(1/\phi_l, 1/\phi_l)$ as prior for both $L, F$.  Note that each grid component has $E = 1, Var = \phi_L$

### initialization
I initialized with 50 runs of `NNLM::nnmf` (`scd`). Then I used medians of each row of $L, F$ as $l_{i0}, f_{j0}$, and $l_{ik} = l^0_{ik}/l_{i0}, f_{jk} = f^0_{jk}/f_{j0}$. 

```{r include=FALSE}
#knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```


```{r}
library(pheatmap)
library(gridExtra)
source("code/misc.R")
source("code/util.R")

output_dir = "output/uci_BoW/v0.3.9/"
data_dir = "data/uci_BoW/"
model_name = "kos_ebpmf_bg_initLF50_K50_maxiter2000.Rds"
model_pmf_name = "kos_pmf_initLF50_K50_maxiter2000.Rds"
dict_name = "vocab.kos.txt"
data_name = "docword.kos.txt"

Y = read_uci_bag_of_words(file= sprintf("%s/%s",
			    data_dir,data_name))
model = readRDS(sprintf("%s/%s", output_dir, model_name))
model_pmf = readRDS(sprintf("%s/%s", output_dir, model_pmf_name))
dict = read.csv(sprintf("%s/%s", data_dir, dict_name), header = FALSE)[,1]
dict = as.vector(dict)

K = ncol(model_pmf$L)
L_pmf = model_pmf$L; F_pmf = model_pmf$F
L_bg = model$l0 * model$qg$qls_mean; F_bg = model$f0 * model$qg$qfs_mean
lf = poisson2multinom(L=L_bg,F=F_bg)
lf_pmf = poisson2multinom(L = L_pmf,F = F_pmf)
```

## ELBO and runtime

```{r}
plot(model$ELBO, xlab = "niter", ylab = "elbo")

## see when it "converges"
plot(model$ELBO[1:200], xlab = "niter", ylab = "elbo")
## ebpmf_bg runtime per iteration
model$runtime/length(model$ELBO)

## pmf runtime per iteration
model_pmf$runtime/length(model_pmf$log_liks)
```

## look at priors in `ebpmf_bg`

### $g_L$
```{r}
get_prior_summary(model$qg$gls)
```

### $g_F$
```{r}
get_prior_summary(model$qg$gfs)
```

## look at $s_k$ (`ebpmf_bg`)
$s_k := \sum_i l_i0 \bar{l}_{ik}$. I make $\sum_j f_{j0} = 1$ for interpretability. 
```{r}
d = sum(model$f0)
s_k = colSums(d * model$l0 * model$qg$qls_mean)
names(s_k) <- paste("Topic", 1:K, sep = "")
step = 5
for(i in 1:round(K/step)){
  print(round(s_k[((i-1)*step + 1):(i*step)]))
}
```

## what does background capture

### compared to rank-1 fit
Is the background very different from the rank-1 model? The rank-1 MLE has $l_{i0} \propto \sum_j X_{ij}$ and $f_{j0} \propto \sum_i X_{ij}$. Let's see if the fitted background model is close to it. 
```{r}
Y_cs = Matrix::colSums(Y)
Y_cs_scaled = Y_cs/sum(Y_cs)
f0_scaled = model$f0/sum(model$f0)
plot(f0_scaled, Y_cs_scaled)

Y_rs = Matrix::rowSums(Y)
Y_rs_scaled = Y_rs/sum(Y_rs)
l0_scaled = model$l0/sum(model$l0)
plot(l0_scaled, Y_rs_scaled)
```

### compared to median/mean of `PMF` fit
The median of $L$ from `PMF` are all 0, so I use mean instead for it
```{r}
f0_pmf = apply(F_pmf, 1, median)
f0_pmf_scaled = f0_pmf/sum(f0_pmf)

l0_pmf = apply(L_pmf, 1, mean)
l0_pmf_scaled = l0_pmf/sum(l0_pmf)

plot(f0_scaled, f0_pmf_scaled)
plot(l0_scaled, l0_pmf_scaled)
```

## Compare $L, F$ (in the context of PMF model)
See [plots](https://zihao12.github.io/ebpmf_data_analysis/kos_K50_v0.3.9_compare_LF.pdf). \

Note: I scale them as below
```{r eval=FALSE}
## scale L, F so that colSums(F) = 1
L_pmf = L_pmf %*% diag(colSums(F_pmf))
F_pmf = F_pmf %*% diag(1/colSums(F_pmf))

L_bg = L_bg %*% diag(colSums(F_bg))
F_bg = F_bg %*% diag(1/colSums(F_bg))
```


## look at top words for topics
See [plots](https://zihao12.github.io/ebpmf_data_analysis/kos_K20_v0.3.9_topic_words.pdf). \

Note: for each topic: \

* the first row selects the top words from $\bar{f}_{Jk}$, and show them in $\bar{f}$(bg) and $f$ (PMF) respectively. \

* The second row shows the top words from $f_{J0}0\bar{f}_{Jk}$ (bg) and $f_{Jk}$ (PMF)\

* The third row transforms $f_{J0}0\bar{f}_{Jk}$ (bg) and $f_{Jk}$ into multinomial model and show their top words.

### take a closer look at some top words
I pick two top words from $\bar{f}_{J1}$ and two top words from $f_{J1}$:
```{r}
d = Matrix::summary(Y)
## `burt`
idx = which(dict == "burt")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]

## `knowles`
idx = which(dict == "knowles")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]

## `campaign`
idx = which(dict == "campaign")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]


## `people`
idx = which(dict == "people")
### number of occurence
sum(d$j == idx)
table(Y[,idx])
### background value
model$f0[idx]
```








