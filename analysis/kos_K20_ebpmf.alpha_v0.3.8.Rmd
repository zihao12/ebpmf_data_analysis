---
title: "kos_K20_ebpmf.alpha_v0.3.8"
author: "zihao12"
date: "2020-05-11"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
I apply `ebpmf.alpha` (version 0.3.8) to [KOS dataset](http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/). I use $K = 20$. The data has $n = 3430,p = 6906$ and sparsity around $98$ percent. 

### model
\begin{align}
    & X_{ij} = \sum_k Z_{ijk}\\
    & Z_{ijk} \sim Pois(l_{i0} f_{j0} l_{ik} f_{jk})\\
    & l_{ik} \sim g_{L, k}(.), f_{jk} \sim g_{F, k}(.) 
\end{align}
For details see [ebpmf_bg](https://github.com/stephenslab/ebpmf.alpha/blob/master/derivations/ebpmf_bg.pdf)

### prior options
I use gamma mixture $\sum_l \pi_{l} Ga(1/\phi_l, 1/\phi_l)$ as prior for both $L, F$.  Note that each grid component has $E = 1, Var = \phi_L$

### initialization
I initialize $l_{i0}, f_{j0}$ by rank-1 pNMF. Then let posterior mean and exp of posterior log mean of $q(l_{ik}), q(f_{jk})$ be all 1. 

```{r include=FALSE}
#knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
```


```{r}
library(pheatmap)
library(gridExtra)
source("code/misc.R")

output_dir = "output/uci_BoW/"
data_dir = "data/uci_BoW/"
model_name = "kos_ebpmf_bg_K20_maxiter5000.Rds"
dict_name = "vocab.kos.txt"
model = readRDS(sprintf("%s/%s", output_dir, model_name))
dict = read.csv(sprintf("%s/%s", data_dir, dict_name), header = FALSE)[,1]
dict = as.vector(dict)
```

## ELBO

* The progress of ELBO (in the first 1000 iterations) is very interesting, different from the curves we see before. \

* From the $g$ below, I guess it is because: $g$ remains the same for many iterations, during which the algo is similar to pNMF (EM) and has siimilar convergence behavior; then $g$ makes one discrete change(mostly), which repeats the convergence behavior ...\

* It might be because of initialization
```{r}
plot(model$ELBO, xlab = "niter", ylab = "elbo")
## runtime
model$runtime
```

## look at $g_L, g_F$

* I show component proportions for $g_L, g_F$. \

*  It is interesting that almost all topics have only one component... (at iteration 500, all topic have one component). Is it because of initialization? Or is the discrete component truly a good global solution? (If that's the case we can just do a grid search instead of solving a convex problem). 
```{r}
get_prior_summary <- function(gs){
  K = length(gs)
  phi_L = gs[[1]][["scale"]]
  idx = order(phi_L, decreasing = TRUE)
  L = length(phi_L)
  Pi = matrix(, nrow = L, ncol = K)
  for(k in 1:K){
    Pi[,k] = gs[[k]][["pi"]][idx]
  }
  rownames(Pi) = paste("phi=", round(phi_L[idx], digits = 4), sep = "")
  colnames(Pi) = paste("Topic", 1:K, sep = "")
  pheatmap(Pi, cluster_rows=FALSE, cluster_cols=FALSE)
}
```

### $g_L$
```{r}
get_prior_summary(model$qg$gls)
```

### $g_F$
```{r}
get_prior_summary(model$qg$gfs)
```

## Some topics are almost the same. 

* Note that a few $(l_k, f_k)$ pairs are very similar to each other (like the first 5 topics are almost identical). Below are the $|\bar{f}_{J,2} - \bar{f}_{J,k}|_1, k = 1,3,4,5$ and $|\bar{l}_{J,2} - \bar{l}_{J,k}|_1, k = 1,3,4,5$. \

* Those almost identical topics/loadings are adjacent in our update order. It seems we need to randomize the update order? 
```{r}
print("|f_2 - f_k|_1, k = 1,3,4,5")
f = model$qg$qfs_mean
sum(abs(f[,2] - f[,1]))
sum(abs(f[,2] - f[,3]))
sum(abs(f[,2] - f[,4]))
sum(abs(f[,2] - f[,5]))

print("|l_2 - l_k|_1, k = 1,3,4,5")
l = model$qg$qls_mean
sum(abs(l[,2] - l[,1]))
sum(abs(l[,2] - l[,3]))
sum(abs(l[,2] - l[,4]))
sum(abs(l[,2] - l[,5]))
```


## quantile of $f_{J0}$
```{r}
f = model$f0
probs = seq(0, 1, 0.002)
plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",0))
```



## quantile of $f_{Jk} (k > 0)$ 
It seems that many topics contain lots of finer structure (not easily explained by a few words). This is not DIFFERENT from what we see from [pNMF results](https://zihao12.github.io/ebpmf_demo/applications_kos.html#look_at_the_meaning_of_each_topic). 

```{r fig.width=14, fig.height=14}
K = length(model$qg$gls)
par(mfrow = c(5,4))
for(k in 1:K){
  f = model$qg$qfs_mean[,k]
  probs = seq(0, 1, 0.002)
  plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",k))
}
```


## quantile of $f_{J0} f_{Jk}$ (scaled to multinom model)
```{r fig.width=14, fig.height=14}
lf = poisson2multinom(F = model$f0 * model$qg$qfs_mean,
                 L = model$l0 * model$qg$qls_mean)

K = length(model$qg$gls)
par(mfrow = c(5,4))
for(k in 1:K){
  f = lf$F[,k]
  probs = seq(0, 1, 0.002)
  plot(probs, quantile(f, probs = probs), main = sprintf("topic %d",k))
}
```

## look at $s_k$
$s_k := \sum_i l_i0 \bar{l}_{ik}$. I make $\sum_j f_{j0} = 1$ for interpretability. 
```{r}
d = sum(model$f0)
s_k = colSums(d * model$l0 * model$qg$qls_mean)
names(s_k) <- paste("Topic", 1:K, sep = "")
#round(s_k[1:5], digits = 0)
step = 5
for(i in 1:round(K/step)){
  print(round(s_k[((i-1)*step + 1):(i*step)]))
}
```



## look at top words for topics

### look at the top words in $\bar{f}_{Jk}$. 
```{r fig.width=16, fig.height=16}
K_sub <- 1:K
#par(mfrow = c(5,4))
p = length(model$l0)
n_top_word = round(0.002 * p)
f = model$qg$qfs_mean

show_topic <- function(k){
  #print(sprintf("topic %d", k))
  word_idx = order(f[,k], decreasing = TRUE)[1:n_top_word]
  F_sub = model$qg$qfs_mean[word_idx,]
  rownames(F_sub) = dict[word_idx]
  colnames(F_sub) = paste("Topic", 1:K, sep = "")
  pheatmap(F_sub, 
           cluster_rows=FALSE, cluster_cols=FALSE,
           silent = TRUE, 
           main = sprintf("topic %d", k))[[4]]
}
gs = lapply(K_sub, FUN = show_topic)
grid.arrange(grobs = gs, ncol = 4)
```

### look at the top words in $f_{J0}\bar{f}_{Jk}$ (after scaling to multinom model). 
```{r fig.width=16, fig.height=16}
K_sub <- 1:K
#par(mfrow = c(5,4))
p = length(model$l0)
n_top_word = round(0.002 * p)
f = lf$F

show_topic <- function(k){
  #print(sprintf("topic %d", k))
  word_idx = order(f[,k], decreasing = TRUE)[1:n_top_word]
  F_sub = model$qg$qfs_mean[word_idx,]
  rownames(F_sub) = dict[word_idx]
  colnames(F_sub) = paste("Topic", 1:K, sep = "")
  pheatmap(F_sub, 
           cluster_rows=FALSE, cluster_cols=FALSE,
           silent = TRUE, 
           main = sprintf("topic %d", k))[[4]]
}
gs = lapply(K_sub, FUN = show_topic)
grid.arrange(grobs = gs, ncol = 4)
```


<!-- ```{r} -->
<!-- f = model$qg$qfs_mean -->
<!-- rownames(f) = NULL -->
<!-- colnames(f) = paste("Topic", 1:K, sep = "") -->
<!-- pheatmap(f) -->
<!-- ``` -->

